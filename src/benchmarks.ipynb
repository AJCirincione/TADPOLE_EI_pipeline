{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 16:46:53.917616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 16:46:54.100480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-27 16:46:54.100516: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-27 16:46:54.146699: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 16:46:55.383935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-27 16:46:55.384092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-27 16:46:55.384112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras import Model\n",
    "from keras.layers.core.dense import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle\n",
    "from keras.backend import dropout\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('ei_python/')\n",
    "from src.src_utils import sample, fmeasure_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '../data/processed/tadpole_data_imptn_norm.pickle'\n",
    "with open(data_file_path, 'rb') as file:\n",
    "    tadpole_data = pickle.load(file)\n",
    "\n",
    "label_file_path = '../data/processed/tadpole_labels_imptn_norm.pickle'\n",
    "with open(label_file_path, 'rb') as file:\n",
    "    tadpole_labels = pickle.load(file)\n",
    "\n",
    "#Dict to save scores:\n",
    "score_dict = {'Autoencoder':{'CV':{}, 'Test':{}}, 'XGB':{'CV':{}, 'Test':{}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'mci_bl'\n",
    "data_split = 'train'\n",
    "concatenated = {}\n",
    "pd.DataFrame()\n",
    "for div in ['train', 'test']:\n",
    "    concatenated[div] = pd.DataFrame()\n",
    "    for mode in tadpole_data[problem][data_split]:\n",
    "        temp = pd.DataFrame(tadpole_data[problem][div][mode])\n",
    "        concatenated[div] = pd.concat([concatenated[div], temp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ae(layer_size, input_dim,  \n",
    "              loss_fn, optimizer, dropout_rate=0, lr=.001):\n",
    "\n",
    "  input_layer = Input(shape = (input_dim, ))\n",
    "  noise = Dropout(dropout_rate)(input_layer)\n",
    "  encoder = layers.Dense(layer_size, activation = 'relu')\n",
    "  encoded = encoder(noise)\n",
    "  #decoder = Dense(input_dim, activation = 'relu')(encoder)\n",
    "  decoder = DenseTranspose(encoder, activation='relu')(encoded)\n",
    "  autoencoder = Model(inputs = input_layer, outputs = decoder)\n",
    "  autoencoder.compile(loss=loss_fn, \n",
    "                      optimizer=keras.optimizers.Adam(\n",
    "                          learning_rate=lr))\n",
    "\n",
    "  return autoencoder\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 45:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "   \n",
    "def ae_fit_and_predict(ae, input, epochs, batch_size, checkpoint_filepath = '/home/opc/model_checkpoints/model_checkpoints_subfolder/'):\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "    callbacks = LearningRateScheduler(scheduler)\n",
    "\n",
    "    stack = ae.fit(input, input, epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[model_checkpoint_callback])\n",
    "    ae.load_weights(checkpoint_filepath)\n",
    "    predict = ae.predict(input, verbose=0)\n",
    "    #del_checkpoints()\n",
    "    return predict\n",
    "\n",
    "def del_checkpoints(checkpoint_filepath = '/home/opc/model_checkpoints/model_checkpoints_subfolder/'):\n",
    "    for filename in os.listdir(checkpoint_filepath):\n",
    "        if ('data' in filename) or ('index' in filename) or ('checkpoint' in filename):\n",
    "            os.remove(checkpoint_filepath + filename)\n",
    "    return \n",
    "\n",
    "def classifier(classifier_input, y, input_dim, epochs, \n",
    "               activation='sigmoid'):\n",
    "\n",
    "  classifier_input_layer = Input(shape = (input_dim, ))\n",
    "  classifier_act = Dense(1, activation=activation)(classifier_input_layer)\n",
    "  classifier = Model(inputs = classifier_input_layer, outputs=classifier_act)\n",
    "  classifier.compile(loss='binary_crossentropy', \n",
    "                     optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "  return classifier\n",
    "\n",
    "# Adapted from \n",
    "# https://medium.com/@sahoo.puspanjali58/a-beginners-guide-to-build-stacked-autoencoder-and-tying-weights-with-it-9daee61eab2b\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "  def __init__(self, dense, activation=None, **kwargs):\n",
    "      self.dense = dense\n",
    "      self.activation = keras.activations.get(activation)\n",
    "      super().__init__(**kwargs)\n",
    "  def build(self, batch_input_shape):\n",
    "      self.biases = self.add_weight(name=\"bias\", \n",
    "                  initializer=\"zeros\",shape=[self.dense.input_shape[-1]])\n",
    "      super().build(batch_input_shape)\n",
    "  def call(self, inputs):\n",
    "      z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "      return self.activation(z + self.biases)  \n",
    "  \n",
    "def ba_max(y_true, y_pred):\n",
    "    one_minus_specs, sensitivity, thresholds = roc_curve(y_true, y_pred)\n",
    "    specificity = 1 - one_minus_specs\n",
    "    balance_accs = (specificity+sensitivity)/2\n",
    "    np_balance_accs = np.array(balance_accs)\n",
    "    max_thres = thresholds[np.argmax(np_balance_accs)]\n",
    "    return max(balance_accs), max_thres\n",
    "\n",
    "def sens_and_spec(y_true, y_pred):\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    thresh = ba_max(y_true, y_pred)[1]\n",
    "    y_pred[np.where(y_pred>=thresh)] = 1\n",
    "    y_pred[np.where(y_pred<thresh)] = 0\n",
    "    t_pos = len(np.intersect1d(np.where(y_true==1)[0],np.where(y_pred==1)[0]))\n",
    "    t_neg = len(np.intersect1d(np.where(y_true==0)[0],np.where(y_pred==0)[0]))\n",
    "    f_pos = len(np.intersect1d(np.where(y_true==0)[0],np.where(y_pred==1)[0]))\n",
    "    f_neg = len(np.intersect1d(np.where(y_true==1)[0],np.where(y_pred==0)[0]))\n",
    "    sens = t_pos / (t_pos + f_neg)\n",
    "    spec = t_neg / (t_neg + f_pos)\n",
    "    return sens, spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare initial params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "optimizer = 'adam'\n",
    "loss_fn = keras.losses.MeanSquaredError( ) \n",
    "epochs = 60\n",
    "lr_factor = 0.01\n",
    "\n",
    "x = concatenated['train']\n",
    "x_test = concatenated['test']\n",
    "\n",
    "y = tadpole_labels['mci_bl']['train'].map({'MCI': 0, 'DEM': 1})\n",
    "y = y.reset_index(drop=True)\n",
    "y_test = tadpole_labels['mci_bl']['test'].map({'MCI': 0, 'DEM': 1})\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "#x_train, y = shuffle(x, y)\n",
    "x_train = x\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state = 42)\n",
    "\n",
    "input_size = x_train.shape\n",
    "input_dim = input_size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ae_preds = {}\n",
    "all_ae_scores = {}\n",
    "hyperparams = {'initial_tuning': [0.003, 0.001], #'fine_tuning': [.003, .001], \n",
    "               'dropout':[0, 0.2, 0.4, 0.6], 'hidden_layer1': [300, 200], \n",
    "               'hidden_layer2': [150, 100], 'hidden_layer3': [100, 75, 50]}\n",
    "\n",
    "y_pred = np.zeros(y.shape)\n",
    "tuned_hyperparams = {}\n",
    "tuning_stages = range(4)\n",
    "for stage in tuning_stages:\n",
    "    print(f'\\n---------- STAGE {stage} ----------\\n')\n",
    "    tuned_hyperparams[stage] = {}\n",
    "    for param in hyperparams:\n",
    "        print(f'\\n--- PARAMETER = {param} ---\\n')\n",
    "        current_params = {}\n",
    "\n",
    "        for other_params in hyperparams:\n",
    "                if (other_params != param) and (stage == 0):\n",
    "                    if other_params in tuned_hyperparams[stage].keys():\n",
    "                        current_params[other_params] = tuned_hyperparams[stage][other_params]\n",
    "                    else:\n",
    "                        current_params[other_params] = hyperparams[other_params][0]\n",
    "                if (other_params != param) and (stage > 0):\n",
    "                    if other_params in tuned_hyperparams[stage].keys():\n",
    "                        current_params[other_params] = tuned_hyperparams[stage][other_params]\n",
    "                    else:  \n",
    "                        current_params[other_params] = tuned_hyperparams[stage-1][other_params]\n",
    "\n",
    "        sos = []    \n",
    "        for p, vals in enumerate(hyperparams[param]):\n",
    "            current_params[param] = vals\n",
    "            \n",
    "            acc_per_fold = []\n",
    "            fold_no = 1\n",
    "            y_pred = np.zeros(y.shape)\n",
    "\n",
    "            for train, test in kfold.split(x_train, y): \n",
    "\n",
    "                #x_sample, y_sample = sample(\n",
    "                        #x_train.iloc[train], y.iloc[train], strategy='undersampling', random_state=42)\n",
    "                x_sample = x_train.iloc[train]\n",
    "                y_sample = y.iloc[train]\n",
    "\n",
    "                autoencoder_1 = create_ae(current_params['hidden_layer1'], dropout_rate=current_params['dropout'], \n",
    "                                        lr=current_params['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "                autoencoder_2 = create_ae(current_params['hidden_layer2'], dropout_rate=current_params['dropout'],\n",
    "                                        lr=current_params['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "                autoencoder_3 = create_ae(current_params['hidden_layer3'], dropout_rate=current_params['dropout'], \n",
    "                                        lr=current_params['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "\n",
    "                ae2_input = ae_fit_and_predict(autoencoder_1, x_sample, epochs = epochs, batch_size=batch_size)\n",
    "                ae3_input = ae_fit_and_predict(autoencoder_2, ae2_input, epochs = epochs, batch_size=batch_size)\n",
    "                classifier_input = ae_fit_and_predict(autoencoder_3, ae3_input, epochs = epochs, batch_size=batch_size)\n",
    "                classifier_model = classifier(classifier_input, y_sample, epochs = epochs, input_dim = input_dim)\n",
    "                final_stack = classifier_model.fit(classifier_input, y_sample, epochs=epochs, \n",
    "                                            batch_size=batch_size, verbose=0)\n",
    "                \n",
    "                scores = classifier_model.evaluate(x_train.iloc[test, :], y[test])\n",
    "                y_pred[test] = classifier_model.predict(x_train.iloc[test, :], verbose=0).T[0]\n",
    "                #print(y_pred)\n",
    "\n",
    "                #acc = [scores[1]]\n",
    "                #acc_per_fold += [acc]\n",
    "                #print(f'\\nClass imbalance ratio = {100 * max(1-y[train].sum(), y[train].sum())/len(y[train])}\\n')\n",
    "                fold_no += 1\n",
    "\n",
    "            acc = sum(1 for a,b in zip(y,[round(prediction) for prediction in y_pred]) if a == b) / len(y)\n",
    "            sos.append(acc)\n",
    "        tmp = max(sos)\n",
    "        idx = sos.index(tmp)\n",
    "        tuned_hyperparams[stage][param] = hyperparams[param][idx]\n",
    "        print(hyperparams[param])\n",
    "        print(sos)\n",
    "        print(f'Best val for {param} is {tuned_hyperparams[stage][param]}')\n",
    "        current_params[param]\n",
    "\n",
    "with open(f'AE_params.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_hyperparams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7704\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7111\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7910\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.6791\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7761\n"
     ]
    }
   ],
   "source": [
    "with open(f'AE_params.pkl', 'rb') as f:\n",
    "    tuned_hyperparams = pickle.load(f)\n",
    "\n",
    "y_pred = np.zeros(y.shape)\n",
    "fold_no = 0\n",
    "for train, test in kfold.split(x_train, y): \n",
    "\n",
    "    #x_sample, y_sample = sample(\n",
    "            #x_train.iloc[train], y.iloc[train], strategy='undersampling', random_state=42)\n",
    "    x_sample = x_train.iloc[train]\n",
    "    y_sample = y.iloc[train]\n",
    "\n",
    "    autoencoder_1 = create_ae(tuned_hyperparams[3]['hidden_layer1'], dropout_rate=tuned_hyperparams[3]['dropout'], \n",
    "                            lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "    autoencoder_2 = create_ae(tuned_hyperparams[3]['hidden_layer2'], dropout_rate=tuned_hyperparams[3]['dropout'],\n",
    "                            lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "    autoencoder_3 = create_ae(tuned_hyperparams[3]['hidden_layer3'], dropout_rate=tuned_hyperparams[3]['dropout'], \n",
    "                            lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "\n",
    "    ae2_input = ae_fit_and_predict(autoencoder_1, x_sample, epochs = epochs, batch_size=batch_size)\n",
    "    ae3_input = ae_fit_and_predict(autoencoder_2, ae2_input, epochs = epochs, batch_size=batch_size)\n",
    "    classifier_input = ae_fit_and_predict(autoencoder_3, ae3_input, epochs = epochs, batch_size=batch_size)\n",
    "    classifier_model = classifier(classifier_input, y_sample, epochs = epochs, input_dim = input_dim)\n",
    "    final_stack = classifier_model.fit(classifier_input, y_sample, epochs=epochs, \n",
    "                                batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    scores = classifier_model.evaluate(x_train.iloc[test], y.iloc[test])\n",
    "    y_pred[test] = classifier_model.predict(x_train.iloc[test], verbose=0).T[0]\n",
    "\n",
    "    for layer in classifier_model.layers:\n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.assign(tf.zeros_like(layer.kernel))\n",
    "\n",
    "    \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores:\n",
      "fmax (minority): 0.6599664991624791\n",
      "AUC: 0.8090431456819818\n",
      "f (majority): 0.7282463186077643\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Print scores\n",
    "fmax_dict = fmeasure_score(y, y_pred)\n",
    "fmax = fmax_dict['F']\n",
    "thres = fmax_dict['thres']\n",
    "f = fmeasure_score(y, y_pred, thres=1-thres, pos_label=0)['F']\n",
    "auc = roc_auc_score(y, y_pred)\n",
    "cv_scores = {'Fmax':fmax, 'f (majority)':f, 'AUC':auc, 'thres':thres}\n",
    "print(f'CV scores:\\nfmax (minority): {fmax}\\nAUC: {auc}\\nf (majority): {f}')\n",
    "score_dict['Autoencoder']['CV']['fmax (minority)'] = fmax\n",
    "score_dict['Autoencoder']['CV']['AUC'] = auc\n",
    "score_dict['Autoencoder']['CV']['f (majority)'] = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'AE_params.pkl', 'rb') as f:\n",
    "    tuned_hyperparams = pickle.load(f)\n",
    "\n",
    "y_test_pred = np.zeros(y_test.shape)\n",
    "\n",
    "autoencoder_1 = create_ae(tuned_hyperparams[3]['hidden_layer1'], dropout_rate=tuned_hyperparams[3]['dropout'], \n",
    "                        lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "autoencoder_2 = create_ae(tuned_hyperparams[3]['hidden_layer2'], dropout_rate=tuned_hyperparams[3]['dropout'],\n",
    "                        lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "autoencoder_3 = create_ae(tuned_hyperparams[3]['hidden_layer3'], dropout_rate=tuned_hyperparams[3]['dropout'], \n",
    "                        lr=tuned_hyperparams[3]['initial_tuning'], input_dim = input_dim, loss_fn = loss_fn, optimizer = optimizer)\n",
    "\n",
    "ae2_input = ae_fit_and_predict(autoencoder_1, x_train, epochs = epochs, batch_size=batch_size)\n",
    "ae3_input = ae_fit_and_predict(autoencoder_2, ae2_input, epochs = epochs, batch_size=batch_size)\n",
    "classifier_input = ae_fit_and_predict(autoencoder_3, ae3_input, epochs = epochs, batch_size=batch_size)\n",
    "classifier_model = classifier(classifier_input, y_sample, epochs = epochs, input_dim = input_dim)\n",
    "final_stack = classifier_model.fit(classifier_input, y, epochs=epochs, \n",
    "                            batch_size=batch_size, verbose=0)\n",
    "\n",
    "#scores = classifier_model.evaluate(x_test, y_test)\n",
    "y_test_pred = classifier_model.predict(x_test, verbose=0).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test scores:\n",
      "f (minority): 0.6119402985074627\n",
      "AUC: 0.7885338345864661\n",
      "f (majority): 0.7450980392156864\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "f_dict = fmeasure_score(y_test, y_test_pred, thres = cv_scores['thres'])\n",
    "fminority = f_dict['F']\n",
    "fmajority = fmeasure_score(y_test, y_test_pred, thres=1-cv_scores['thres'], pos_label=0)['F']\n",
    "auc = roc_auc_score(y_test, y_test_pred)\n",
    "test_scores = {'Fmax':fminority, 'f (majority)':fmajority, 'AUC':auc}\n",
    "print(f'Test scores:\\nf (minority): {fminority}\\nAUC: {auc}\\nf (majority): {fmajority}')\n",
    "score_dict['Autoencoder']['Test']['f (minority)'] = fminority\n",
    "score_dict['Autoencoder']['Test']['AUC'] = auc\n",
    "score_dict['Autoencoder']['Test']['f (minority)'] = fmajority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV scores: $\\\\$\n",
    "fmax (minority): 0.6599664991624791\n",
    "AUC: 0.8090431456819818\n",
    "f (majority): 0.7282463186077643\n",
    "\n",
    "Test scores: $\\\\$\n",
    "f (minority): 0.6119402985074627\n",
    "AUC: 0.7885338345864661\n",
    "f (majority): 0.7450980392156864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('ei_python/')\n",
    "from src.ei_python.ei import EnsembleIntegration,  MeanAggregation, MedianAggregation\n",
    "from src.ei_python.utils import f_minority_score\n",
    "from src.src_utils import EI_model_train_and_save, fmeasure_score\n",
    "from src.ei_python.interpretation import *\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import copy\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, Normalizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, \\\n",
    "    matthews_corrcoef, precision_recall_fscore_support, make_scorer\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##################################################################################################\n",
      "######################################## unimode modality ########################################\n",
      "################################################################################################## \n",
      "\n",
      "\n",
      "Training base predictors and generating data for analysis...\n",
      "Generating meta training data via nested cross validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base predictors on outer training sets...\n",
      "\n",
      "Base predictor training is complete: see \"base_summary\" attribute for a summary of base predictor performance. Meta training data can be found in \"meta_training_data\" and \"meta_test_data\" attributes. Run \"train_meta\" method for analysis of ensemble algorithms.\n",
      "\n",
      "Training base predictors and generating data for final ensemble...\n",
      "Generating meta training data via nested cross validation...\n",
      "Training base predictors on outer training sets...\n",
      "\n",
      "Model building: meta training data for the final model has been generated and can be found in the \"meta_training_data_final\" attribute. Final base predidctors have been saved in the \"final_models\" attribute.\n",
      "\n",
      "Saved to EI.XGB_benchmark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "problem = 'mci_bl'\n",
    "data_split = 'train'\n",
    "est = 100\n",
    "\n",
    "y = tadpole_labels['mci_bl']['train'].map({'MCI': 0, 'DEM': 1})\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "x = np.array(concatenated[data_split])\n",
    "y = np.array(tadpole_labels[problem][data_split].map({'MCI': 0.0, 'DEM': 1.0}))\n",
    "\n",
    "EI_model_train_and_save(f'XGB_benchmark', single_mode=True, meta_training=False,\n",
    "                        base_predictors={\"XGB\": XGBClassifier(n_estimators=est)},\n",
    "                        mode_dict = x, \n",
    "                        y = y.astype(int), train = True, random_state=41, \n",
    "                        model_building=True, sampling_strategy=None,\n",
    "                        n_samples=1)\n",
    "\n",
    "XGB = EnsembleIntegration.load(f'EI.XGB_benchmark')\n",
    "for metrics in ['fmax (minority)','AUC',  'f (majority)']:\n",
    "    score_dict['XGB']['CV'][metrics] = XGB.base_summary['metrics']['unimode'].T[metrics][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = concatenated['test']\n",
    "y_test = np.array(tadpole_labels['mci_bl']['test'].map({'MCI': 0.0, 'DEM': 1.0}))\n",
    "#y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "XGB = EnsembleIntegration.load(f'EI.XGB_benchmark')\n",
    "base_models_list = copy.deepcopy(XGB.final_models[\"base models\"][\"unimode\"])\n",
    "XGB_dicts = [dictionary for dictionary in base_models_list if dictionary[\"model name\"] == \"XGB\"]\n",
    "XGB_model = pickle.loads(XGB_dicts[0][\"pickled model\"])\n",
    "fmax_minor_thresh = XGB.base_summary['thresholds']['unimode']['XGB'][0]\n",
    "test_preds = XGB_model.predict(x_test)\n",
    "\n",
    "score_dict['XGB']['Test']['f (minority)'] = fmeasure_score(y_test, test_preds, pos_label=1, thres = fmax_minor_thresh)['F']\n",
    "score_dict['XGB']['Test']['AUC'] = roc_auc_score(y_test, test_preds)\n",
    "score_dict['XGB']['Test']['f (majority)'] = fmeasure_score(y_test, test_preds, pos_label=0, thres = 1-fmax_minor_thresh)['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['Autoencoder']['CV']['fmax (minority)'] = 0.6599664991624791\n",
    "score_dict['Autoencoder']['CV']['AUC'] = 0.8090431456819818\n",
    "score_dict['Autoencoder']['CV']['f (majority)'] = 0.7282463186077643\n",
    "\n",
    "score_dict['Autoencoder']['Test']['f (minority)'] = 0.6119402985074627\n",
    "score_dict['Autoencoder']['Test']['AUC'] = 0.7885338345864661\n",
    "score_dict['Autoencoder']['Test']['f (majority)'] = 0.7450980392156864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('benchmark_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(score_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
